{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANocracy: A Practioner's Guide to Training GANs in 2019\n",
    "\n",
    "**Author**: `Alex Andonian`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will give a brief introduction to some of the more recent achievements of Generative Adversarial Networks (GANs), which have seen tremendous progress in their short 5 year lifespan:\n",
    "\n",
    "![GAN Progress in Image Synthesis](assets/progress.jpg)\n",
    "\n",
    "\n",
    "As impressive as these results are, training the latest state-of-the-art GANs isn't without its challenges. For example GANs are still: \n",
    " - sensitivive to structure and parameters/configuration\n",
    " - susceptible to model collapse\n",
    " - demanding on computational resources, especially for larger resolutions\n",
    " \n",
    "The goal of this tutorial is to prepare users to take on the task of training large-scale, high-resolution GANs of their own. We will begin training a simplified version of the recent BigGAN architecture and walk through some techniques for monitoring and debugging the training process. Finally, we will demonstrate how to control sample generation using a fully pretrained BigGAN.\n",
    "\n",
    "This tutorial assumes a basic understanding of GANs as well as a machine ready to run the latest release of PyTorch (1.1). For a general overview of GANs and how they work, please refer to [Insert Introduction Link]. For more detailed instructions on how to install PyTorch, please see these instructions [Insert Link to Setup.md].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "For the **casual observer** just curious about learning more about recent GAN developments, this notebook along with the provided links to external references should be sufficient an informative document. A powerful laptop or mainstream desktop computer should be enough for you to execute most of the computationally inexpensive (e.g. inference with pretrained generators) cells.\n",
    "\n",
    "For the **interested practioner** keen on running the full notebook and playing around with different datasets and hyperparameters, a powerful desktop/server with at least one modern GPU is *highly* recommended, if not required. The median recommendation is 4-8 GPUs.\n",
    "\n",
    "If you do not have access to the recommended computational resources, it is possible (and fairly easy) to create a sufficiently powerful Virtual Machine (VM) on one of several cloud providers. We provide brief instructions and suggestions on how to get up and running with the following providers:\n",
    "\n",
    "- [Core Scientific] TODO\n",
    "- [Google Cloud] TODO\n",
    "- [IBM Cloud] TODO\n",
    "\n",
    "Nvidia and AWS also provide excellent options, but we are not able to provide specific instructions at this time.\n",
    "\n",
    "For the **hardcore \"GANologist\"** determined to reproduce (and exceed) the performance of the latest state-of-the-art GANs, we recommend using this notebook as a thorough introduction and reference before stepping up to full-fledged (officially unofficial) implementations such as [BigGAN-PyTorch](https://github.com/ajbrock/BigGAN-PyTorch)\n",
    "\n",
    "### System & Software Requirements\n",
    "Both Linux and Windows machines are supported, but we strongly recommend Linux for performance and compatibility reasons. \n",
    "\n",
    "- PyTorch, version 1.1 (stable)\n",
    "- tqdm, numpy, scipy, h5py\n",
    "\n",
    "### Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# GANOCRACY LIB\n",
    "sys.path.append('../')\n",
    "import ganocracy\n",
    "from ganocracy.data import datasets as dset\n",
    "from ganocracy.data import transforms\n",
    "from ganocracy.utils import visualizer as vutils\n",
    "from ganocracy import metrics\n",
    "\n",
    "# NOTEBOOK-SPECIFIC IMPORTS\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    # DATA CONFIG\n",
    "    'data_root': 'data',            # Root directory where datasets are stored\n",
    "    'dataset': 'ImageNet',          # Name of dataset \n",
    "    'dataset_type': 'ImageHDF5',    # Type/format of dataset class used to load data.\n",
    "    'parallel': True,\n",
    "    'shuffle': True,                # Shuffle training data\n",
    "    'batch_size': 512,              \n",
    "    'num_workers': 8,\n",
    "    'load_in_mem': True,\n",
    "    'resolution': 64,\n",
    "    'download': True,\n",
    "    'split': 'train',\n",
    "    \n",
    "    # MODEL ARCHITECTURE\n",
    "    'G_ch': 96,\n",
    "    'D_ch': 96,\n",
    "    'G_attn': 64,\n",
    "    'D_attn': 64,\n",
    "    'hier': True,\n",
    "    'dim_z': 120,\n",
    "    'shared_dim': 128,\n",
    "    'G_init': 'ortho',\n",
    "    'D_init': 'ortho',\n",
    "    'G_n1': 'inplace_relu',\n",
    "    'D_n1': 'inplace_relu',\n",
    "    'G_eval_mode': True,\n",
    "    \n",
    "    # TRAINING CONFIG     \n",
    "    'num_D_steps': 1,\n",
    "    'num_G_accumulations': 4,\n",
    "    'num_D_accumulations': 4,\n",
    "    'G_lr': 1e-4,\n",
    "    'D_lr': 4e-4,\n",
    "    'D_B2': 0.999,\n",
    "    'G_B2': 0.999,\n",
    "\n",
    "    'ema': True,\n",
    "    'use_ema': True,\n",
    "    'ema_start': 20000,\n",
    "    'test_every': 2000,\n",
    "    'save_every': 1000,\n",
    "    'num_best_copies': 5,\n",
    "    'num_save_copies': 2,\n",
    "    'seed': 0,\n",
    "    'use_multiepoch_sampler': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In this tutorial, we will use the [Places365 dataset](http://places2.csail.mit.edu/) which contains more than 10 million images comprising 400+ unique scene categories. You can manually download the full dataset [here](http://places2.csail.mit.edu/download.html), although we provide standard PyTorch dataloaders that download, cache and preprocess several standard datasets as part of this tutorial. Therefore, preparing a dataloader can be accomplished with just a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name, root_dir=None, resolution=128, filetype='tar'):\n",
    "    if filetype == 'tar':\n",
    "        url = dset.data_urls[name]['tar']\n",
    "        data_dir = dset.load_data_from_url(url, root_dir)\n",
    "        dataset = dset.ImageFolder(root=data_dir,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.CenterCropLongEdge(),\n",
    "                                       transforms.Resize(resolution),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                            (0.5, 0.5, 0.5))\n",
    "                                    ]))\n",
    "    elif filetype == 'hdf5':\n",
    "        url = dset.data_urls[name]['hdf5'][resolution]\n",
    "        hdf5_file = dset.load_data_from_url(url, root_dir)\n",
    "        dataset = dset.ImageHDF5(hdf5_file)\n",
    "    else:\n",
    "        raise ValueError('Unreconized filetype: {}'.format(filetype))\n",
    "    return dataset\n",
    "\n",
    "# dataset = get_dataset(config['dataset'],\n",
    "#                       root_dir=config['data_root'],\n",
    "#                       resolution=config['resolution'])\n",
    "\n",
    "# dataloader = torch.utils.data.DataLoader(dataset,\n",
    "#                                          shuffle=config['shuffle'],\n",
    "#                                          batch_size=config['batch_size'],\n",
    "#                                          num_workers=config['num_workers'])\n",
    "# vutils.visualize_data(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolution = config['resolution']\n",
    "# dataset = torchvision.datasets.ImageNet(config['data_root'],\n",
    "#                                         split=config['split'],\n",
    "#                                         download=config['download'],\n",
    "#                                         transform=transforms.Compose([\n",
    "#                                             transforms.CenterCropLongEdge(),\n",
    "#                                             transforms.Resize(resolution),\n",
    "#                                             transforms.ToTensor(),\n",
    "#                                             transforms.Normalize((0.5, 0.5, 0.5),\n",
    "#                                                                  (0.5, 0.5, 0.5))\n",
    "#                                     ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will need to organize and preprocess the training data that will serve as the 'real' examples presented to the discriminator. If you are training an unconditional GAN with only a single class of images, it would be enough to simply store all of your examples in a directory like so:\n",
    "\n",
    "    satellite_images/34_-77_2016-02-28.png\n",
    "    satellite_images/34_-77_2014-06-29.png\n",
    "    satellite_images/36_-75_2008-07-13.png\n",
    "    satellite_images/30_-84_2010-07-11.png\n",
    "    satellite_images/27_-97_2010-07-18.png\n",
    "    \n",
    "If you are training a class-conditional GAN such as SA-GAN or BigGAN, it is fairly common to store images organized as so:\n",
    "\n",
    "    root/dogball/xxx.png\n",
    "    root/dogball/xxy.png\n",
    "    root/dogball/xxz.png\n",
    "\n",
    "    root/cat/123.png\n",
    "    root/cat/nsdf3.png\n",
    "    root/cat/asd932_.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data as single HDF5 file (optional, for additional performance)\n",
    "\n",
    "If your dataset consists of large, high-resolution images, then repeatedly applying transforms to the raw images (cropping, resizing) results in many wasted CPU cycles. Furthermore, increasing your batchsize puts additional I/O strain on your filesystem. Together, these factors may produce a dataloading bottleneck where your GPUs consume data faster than your system can produce it. To remedy this, you may choose to prepare a pre-processed HDF5 version of your target dataset using the utilities provided. Moreover, if I/O still appears to be the bottleneck, you may choose to load the entire dataset into RAM by setting `load_in_mem` dataset kwargs to `True` (if your system can support this).\n",
    "\n",
    "Another advantage to using a single, preprocessed HDF5 is the ease with which you can transfer data to multiple remote machines, which normally becomes cumbersome and time consuming when attempting large, distributed experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_name = '{}-{}.hdf5'.format(config['dataset'], config['resolution'])\n",
    "print(hdf5_name)\n",
    "hdf5_file = os.path.join(config['data_root'], hdf5_name)\n",
    "print(hdf5_file)\n",
    "# hdf5_file = dset.make_hdf5(dataloader, config['data_root'], hdf5_name)\n",
    "dataset = dset.ImageHDF5(hdf5_file, load_in_mem=config['load_in_mem'])\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         shuffle=config['shuffle'],\n",
    "                                         batch_size=config['batch_size'],\n",
    "                                         num_workers=config['num_workers'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to measure sample quality during training\n",
    "\n",
    "Typically, when training any sort of neural network, it is standard practice to monitor the value of the objective function (loss) throughout the course of the experiment. With GANs, it is also common to break the loss into individual components.\n",
    "\n",
    "Objectively evaluating implicit generative models is difficult (Theis et al., 2015). A variety of works\n",
    "have proposed heuristics for measuring the sample quality of models without tractable likelihoods\n",
    "(Salimans et al., 2016; Heusel et al., 2017; Binkowski et al., 2018; Wu et al., 2017). Of these, ´\n",
    "the Inception Score (IS, Salimans et al. (2016)) and Frechet Inception Distance (FID, Heusel et al. ´\n",
    "(2017)) have become popular despite their notable flaws (Barratt & Sharma, 2018). We employ\n",
    "them as approximate measures of sample quality, and to enable comparison against previous work.\n",
    "\n",
    "While training, it would be very useful to objectively measure the quality of generated samples in order to understand how training is progressing (without having to manually assign subjective ratings to a large. Moreover,  To this end, researchers have developed metrics which seek to capture the quality of the generated samples, with the two most popular being:\n",
    "- Inception Score (IS)\n",
    "- Frechet Distance (FID)\n",
    "\n",
    "\n",
    "Higher IS values mean better image quality, but not necessarily diversity.\n",
    "Lower FID values mean better image quality and diversity.\n",
    "\n",
    "or datasets other than ImageNet, Inception Score can be a very poor measure of quality, so you will likely want to use --which_best FID instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inception_moments_file = metrics.calculate_inception_moments(dataloader, config['data_root'], config['dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "\n",
    "## Convolutional GANS \n",
    "\n",
    "[Deep Convolutional GANS](https://arxiv.org/abs/1511.06434) (DCGANs) represented a major step forward in the sucess of GAN image synthesis. The introduction of convolutional layers, which had already proven successful for disciminative computer vision tasks\n",
    "Structure well suited for image generatation.\n",
    "When applied to images, G and D are usually convolutional neural networks (Radford et al., 2016).\n",
    "![DCGAN Generator Architecure](assets/dcgan_generator.png)\n",
    "\n",
    "Almost all GANs used for image synthesis now follow some variation of DCGAN.\n",
    "- GBlock\n",
    "- DBlock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator consists of GBlocks, which progressively increase the spatial dimensions while decreasing the feature volume depth.\n",
    "\n",
    "Batch norm follows the transposed Convolution, which works to combat poor initialization schemes and mode collapes. *Recall, batch norm works by normalizing the inputs features to have zero mean and unit variance.*\n",
    "\n",
    "*Mode Collapse*: When the generator falls into a situation where it produces an extremely limited set output patterns (\"modes\") despite maintaining diversity on input noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward method of GBlock.\n",
    "        \n",
    "        This block increases the spatial resolution by 2:\n",
    "        \n",
    "            input:  [batch_size, in_channels, H, W]\n",
    "            output: [batch_size, out_channels, 2*H, 2*W]\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.conv(input)\n",
    "        x = self.bn(x)\n",
    "        out = self.act(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBlocks, found in the discriminator, are near perfect inverses to GBlocks, trading spatial dimension for feature depth, with the exception of using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward method of DBlock.\n",
    "        \n",
    "        This block decreases the spatial resolution by 2:\n",
    "        \n",
    "            input:  [batch_size, in_channels, H, W]\n",
    "            output: [batch_size, out_channels, H/2, W/2]\n",
    "        \"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GANs\n",
    "Produce images of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider swapping ccbn and SN\n",
    "\n",
    "## Spectral Normalization (Stability)\n",
    "\n",
    "https://arxiv.org/abs/1802.05957\n",
    "\n",
    "\n",
    "**Problem**: GAN training is highly dynamic ()\n",
    "\n",
    "Miyato et al proposed a normalization technique called spectral normalization (SN), which enforces Lipschitz continuity on the weights of the discriminator (and generator).\n",
    "\n",
    "Briefly, Lipschitz continuity relates to how quickly a function can change. For example, every function that has bounded first derivatives is Lipschitz, and the smallest such bound is called the Lipschitz constant. \n",
    "\n",
    "Spectral normed layers enforce Lipschitz continuity by normalizing its parameters with running estimates of their frist singular values.  These estimates can be computed using the power iteration method described below:\n",
    "\n",
    "![spectral normalization algorithm](assets/spectral_norm_algorithm.png)\n",
    "\n",
    "Originally shown to be useful in D, but later also shown to improve stability in G, allowing for fewer D steps per iteration.\n",
    "(**Note**: Numerous t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Mechanism (Diversity)\n",
    "\n",
    "First proposed by the GANfather, Ian Goodfellow himself, in the paper [Self-Attention Generative Adversarial Networks](https://arxiv.org/abs/1805.08318)(SA-GANs), the introduction of a self-attention mechanism to GANs is aimed at\n",
    "\n",
    "### Observation and Inspiration\n",
    "Prior to SA-GANs, researchers noticed that while vanilla DCGAN-style GANs continued to improve on datasets with limited number of classes (such as faces), they still stuggled to learn the image distributions of diverse multi-class datasets like Imagenet. \n",
    "\n",
    "convolutions are (spatially) local operations, with the extend of spatial dependencies (receptive field) limited by the kernel size. Thus, capturing long-range (non-local) dependenices is not really possible since the outputs of a convolution at one spatial location have no effect on the outputs at another distant location.\n",
    "\n",
    "Possible solutions:\n",
    "\n",
    "- increase kernel size (slow and inefficient)\n",
    "- deeper network (more layers) so that later layers have larger receptive fields (more parameters, more difficult to train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Self-Attention GAN regions attended](assets/sagan_regions.png)\n",
    "\n",
    "\n",
    "![Self-Attention Block](assets/sa_block.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scaling Up GANs\n",
    "\n",
    "### Batch Size\n",
    "Simply increasing the batch size by a factor of 8 improves the state-of-the-art IS by 46%. We conjecture that this is a result of each batch covering more modes, providing better gradients for both networks.\n",
    "\n",
    "### Number of channels\n",
    "We then increase the width (number of channels) in each layer by 50%, approximately doubling the\n",
    "number of parameters in both models. This leads to a further IS improvement of 21%, which we\n",
    "posit is due to the increased capacity of the model relative to the complexity of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two-timescale update rule (TTUR)\n",
    "\n",
    "## Exponential Moving Average (EMA)\n",
    "We use an exponential moving average of the weights of G at sampling time, with a decay rate set to\n",
    "0.9999\n",
    "\n",
    "## Gradient Accumulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "![BigGAN Architecture](assets/biggan_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    # Maps output resoluton to number of GBlocks.\n",
    "    res2blocks = {\n",
    "        32: 3,\n",
    "        64: 4,\n",
    "        128: 5,\n",
    "        256: 6,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, dim_z=128, resolution=128, G_ch=64, \n",
    "                block=GBlock):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.G_ch = G_ch\n",
    "        self.dim_z = dim_z\n",
    "\n",
    "        self.num_blocks = self.res2blocks[resolution]\n",
    "        self.ch_nums = [2**i for i in range(self.num_blocks)]\n",
    "        self.ch_nums += self.ch_nums[-1:]\n",
    "        self.ch_nums = list(reversed(self.ch_nums))\n",
    "        \n",
    "        self.linear = nn.Linear(dim_z, G_ch * self.ch_nums[0] * 4**2)\n",
    "        self.GBlocks = nn.Sequential(*[\n",
    "            block(G_ch * in_c, G_ch * out_c)\n",
    "            for in_c, out_c in zip(self.ch_nums, self.ch_nums[1:])\n",
    "        ])\n",
    "        self.out = nn.Conv2d(G_ch * 1, 3, 3, padding=1)  # RGB image has 3 channels\n",
    "        self.tanh = nn.Tanh()                            # \"Squashes\" out to be in range[-1, 1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(x.size(0), -1, 4, 4)\n",
    "        x = self.GBlocks(x)\n",
    "        return self.tanh(self.out(x))\n",
    "    \n",
    "dim_z = 128\n",
    "z = torch.rand(10, config['dim_z'])\n",
    "G = Generator(dim_z=config['dim_z'], resolution=config['resolution'])\n",
    "print(G)\n",
    "print('output shape:', G(z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    # Maps output resoluton to number of DBlocks.\n",
    "    res2blocks = {\n",
    "        32: 3,\n",
    "        64: 4,\n",
    "        128: 5,\n",
    "        256: 6,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, resolution=128, D_ch=64, block=DBlock):\n",
    "        super().__init__()\n",
    "        self.D_ch = D_ch\n",
    "        self.num_blocks = self.res2blocks[resolution]\n",
    "        self.fnums = [2**i for i in range(self.num_blocks)]\n",
    "        self.input_layer = nn.Conv2d(3, D_ch, 3, padding=1)\n",
    "        \n",
    "        self.DBlocks = nn.Sequential(*[\n",
    "            block(D_ch * in_c, D_ch * out_c)\n",
    "            for in_c, out_c in zip(self.fnums, self.fnums[1:])\n",
    "        ])\n",
    "        \n",
    "        self.out = nn.Conv2d(D_ch * self.fnums[-1], 1, 3, 1, 0)\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.DBlocks(x)\n",
    "        x = self.act(torch.mean(self.out(x), [2, 3]))\n",
    "        return x\n",
    "\n",
    "\n",
    "x = torch.rand(10, 3, config['resolution'], config['resolution'])\n",
    "D = Discriminator(resolution=config['resolution'])\n",
    "print(D)\n",
    "print('output shape:', D(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_training_function(G, D, GD, z_, y_, ema, state_dict, config):\n",
    "    def train(x, y):\n",
    "        G.optim.zero_grad()\n",
    "        D.optim.zero_grad()\n",
    "        # How many chunks to split x and y into?\n",
    "        x = torch.split(x, config['batch_size'])\n",
    "        y = torch.split(y, config['batch_size'])\n",
    "        counter = 0\n",
    "\n",
    "        # Optionally toggle D and G's \"require_grad\"\n",
    "        if config['toggle_grads']:\n",
    "            utils.toggle_grad(D, True)\n",
    "            utils.toggle_grad(G, False)\n",
    "\n",
    "        ###############################################################\n",
    "        #                    TRAIN DISCRIMINATOR\n",
    "        ###############################################################\n",
    "        for step_index in range(config['num_D_steps']):\n",
    "            # If accumulating gradients, loop multiple times before an optimizer step\n",
    "            for accumulation_index in range(config['num_D_accumulations']):\n",
    "                z_.sample_(), y_.sample_()\n",
    "                D_fake, D_real = GD(z_[:config['batch_size']], y_[:config['batch_size']],\n",
    "                                    x[counter], y[counter], train_G=False,\n",
    "                                    split_D=config['split_D'])\n",
    "\n",
    "                # Compute components of D's loss, average them, and divide by\n",
    "                # the number of gradient accumulations.\n",
    "                D_loss_real, D_loss_fake = losses.discriminator_loss(D_fake, D_real)\n",
    "                D_loss = (D_loss_real + D_loss_fake) / float(config['num_D_accumulations'])\n",
    "                D_loss.backward()\n",
    "                counter += 1\n",
    "\n",
    "            # Optionally apply ortho reg in D.\n",
    "            if config['D_ortho'] > 0.0:\n",
    "                # Debug print to indicate we're using ortho reg in D.\n",
    "                print('using modified ortho reg in D')\n",
    "                utils.ortho(D, config['D_ortho'])\n",
    "\n",
    "            D.optim.step()\n",
    "\n",
    "        # Optionally toggle \"requires_grad\"\n",
    "        if config['toggle_grads']:\n",
    "            utils.toggle_grad(D, False)\n",
    "            utils.toggle_grad(G, True)\n",
    "\n",
    "        # Zero G's gradients by default before training G, for safety.\n",
    "        G.optim.zero_grad()\n",
    "\n",
    "        ###############################################################\n",
    "        #                    TRAIN GENERATOR\n",
    "        ###############################################################\n",
    "        # If accumulating gradients, loop multiple times.\n",
    "        for accumulation_index in range(config['num_G_accumulations']):\n",
    "            z_.sample_(), y_.sample_()\n",
    "            D_fake = GD(z_, y_, train_G=True, split_D=config['split_D'])\n",
    "            G_loss = losses.generator_loss(D_fake) / float(config['num_G_accumulations'])\n",
    "            G_loss.backward()\n",
    "\n",
    "        # Optionally apply modified ortho reg in G.\n",
    "        if config['G_ortho'] > 0.0:\n",
    "            print('using modified ortho reg in G')  # Debug print to indicate we're using ortho reg in G.\n",
    "            # Don't ortho reg shared, it makes no sense. Really we should blacklist any embeddings for this...\n",
    "            utils.ortho(G, config['G_ortho'],\n",
    "                        blacklist=[param for param in G.shared.parameters()])\n",
    "        G.optim.step()\n",
    "\n",
    "        # If we have an ema, update it, regardless of if we test with it or not.\n",
    "        if config['ema']:\n",
    "            ema.update(state_dict['itr'])\n",
    "\n",
    "        out = {'G_loss': float(G_loss.item()),\n",
    "               'D_loss_real': float(D_loss_real.item()),\n",
    "               'D_loss_fake': float(D_loss_fake.item())}\n",
    "        return out\n",
    "    return train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we're off!\n",
    "\n",
    "... And now we wait. At lower resolutions or fewer classes, it is possible to obtain farily respectable results in a short time frames. However, acheiving  the eye-catching results commonly advertised in paper and the media still takes quite a while, on the order of weeks potentially. \n",
    "\n",
    "Below is a table of some fairly common configurations and their expected training times:\n",
    "\n",
    "|INSERT TABLE|\n",
    "\n",
    "### \"Babysitting\" the learning process\n",
    "\n",
    "Given that training these models can be an investment in time and resources, it's wise to continuously monitor training in order to catch and address anamolies if/when they occur. Here are some things to look out for:\n",
    "\n",
    "**At the start of training**\n",
    "\n",
    "- Losses: do they fal Are the models learning? [INSERT LOSS PLOT]\n",
    "- Speed: Based on time per iteration, estimate how long training will take. Is it acceptble to you?\n",
    "- GPU utilization: Are you using GPUs to the fullest? Use command `nvidia-smi` to check on utilization and memory usage. Could you use a larger batch size? Evidence of a dataloading bottleneck? [INSERT GIF of GPU UTILs]\n",
    "\n",
    "**During training**\n",
    "- Are losses still within normal limits? High frequency oscillations are expected.\n",
    "- Monitor IS and FID metrics - are they following the expected trajectories? One of the hardest things about re-implementing a paper can be checking if the logs line up early in training, especially if training takes multiple weeks.\n",
    "- How do the samples look? Are they improving over time? Do you see evidence of mode collapse?\n",
    "- Singular Values of weights?\n",
    "\n",
    "**End of training**\n",
    "- Most importantly, do the samples meet your expectations?\n",
    "- Sharp increase in metrics followed by collapse?\n",
    "- No longer improving.\n",
    "- Explore your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Visualization of G’s progression**\n",
    "\n",
    "Remember how we saved the generator’s output on the fixed_noise batch\n",
    "after every epoch of training. Now, we can visualize the training\n",
    "progression of G with an animation. Press the play button to start the\n",
    "animation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i, (1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Real Images vs. Fake Images**\n",
    "\n",
    "Finally, lets take a look at some real images and fake images side by\n",
    "side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,30))\n",
    "plt.subplot(2,1,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "if img_list:\n",
    "# Plot the fake images from the last epoch\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Fake Images\")\n",
    "    plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading off variety and fidelity with the \"truncation trick\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intra-class (z only) Latent space interpolation\n",
    "num_samples = 4\n",
    "num_midpoints = 8\n",
    "label = 1\n",
    "\n",
    "dev = next(netG.parameters()).device\n",
    "x0 = torch.randn(num_samples, nz).to(dev)\n",
    "x1 = torch.randn(num_samples, nz).to(dev)\n",
    "zs = gvutils.interp(x0, x1, num_midpoints, device=dev)\n",
    "zs = zs.view(-1, zs.size(-1))\n",
    "ys = torch.ones(zs.size(0), device=device) * label\n",
    "samples = netG(zs, ys).detach()\n",
    "\n",
    "plt.figure(figsize=(15,30))\n",
    "plt.subplot(1,1,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(samples, nrow=num_midpoints + 2, padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "print(netG.class_linear.weight.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-wise interpolation\n",
    "# Intra-class (z only) Latent space interpolation\n",
    "num_samples = 4\n",
    "num_midpoints = 8\n",
    "\n",
    "dev = next(netG.parameters()).device\n",
    "x0 = torch.randn(num_samples, nz).to(dev)\n",
    "x1 = torch.randn(num_samples, nz).to(dev)\n",
    "zs = gvutils.interp(x0, x1, num_midpoints, device=dev)\n",
    "zs = zs.view(-1, zs.size(-1))\n",
    "print('zs.size():', zs.size())\n",
    "coastal_embed = netG.shared(torch.ones(num_samples, device=dev) * 0)\n",
    "noncoastal_embed = netG.shared(torch.ones(num_samples, device=dev) * 1)\n",
    "ys = gvutils.interp(coastal_embed, noncoastal_embed, num_midpoints, device=dev)\n",
    "print(ys.shape)\n",
    "ys = ys.view(-1, ys.size(-1))\n",
    "print(ys.shape)\n",
    "\n",
    "samples = netG.generate(zs, ys).detach()\n",
    "\n",
    "plt.figure(figsize=(15,30))\n",
    "plt.subplot(1,1,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(samples, nrow=num_midpoints + 2, padding=5, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
