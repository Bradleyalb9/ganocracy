{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANocracy: A Practioner's Guide to Training GANs in 2019\n",
    "\n",
    "**Author**: `Alex Andonian`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will give a brief introduction to some of the more recent achievements of Generative Adversarial Networks (GANs), which have seen tremendous progress in their short 5 year lifespan. **Fun Fact:** GANs celebrate their 5th anniversary on May 26, 2019. The rapid progress of GANs' sucess in image synthesis is particularly impressive:\n",
    "\n",
    "![GAN Progress in Image Synthesis](assets/progress.jpg)\n",
    "\n",
    "In light of the enormous volume of work devoted to GANs, it is nearly impossible to cover everything. Instead, we will focus on one particular line of research that has recently culminated in state-of the art Imagenet generation.\n",
    "\n",
    "- **Deep Convolutional GANs (DCGANs)**: introduced convolutions to GAN architectures, forming the basis for image GAN architectures to come.\n",
    "- **Conditional GANs (cGANs)**: Generate images of different categories.\n",
    "- **Spectral Normalization (SNGANs)**: Improved training stability.\n",
    "- **Self-Attention Mechanism (SAGANs)** for modeling long-range dependencies and handling diverse multi-class datasets.\n",
    "- **Large Scale GAN training (BigGANs)**: achieves the current state-of-the-art on ImageNet generation. \n",
    "\n",
    "This tutorial is very much *for the practioner, by the practioner*. The goal is to prepare users with the tools and intuitions to take on the task of training large-scale, high-resolution GANs of their own. We will begin training a simplified version of the recent BigGAN architecture and walk through some techniques for monitoring and debugging the training process. Finally, we will demonstrate how to control sample generation using a fully pretrained BigGAN.\n",
    "\n",
    "This tutorial assumes a basic understanding of GANs and how they work. If this is your very first introduction to GANs (and/or you find yourself with a growing list of questions), you may want to first review [PyTorch's DCGAN tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html), to which this serves as an extension. If  as well as a machine ready to run the latest release of PyTorch (1.1). For a general overview of GANs and how they work, please refer to [Insert Introduction Link]. For more detailed instructions on how to install PyTorch, please see these instructions [Insert Link to Setup.md].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get the most out of this tutorial\n",
    "\n",
    "For the **casual observer** just curious about learning more about recent GAN developments, this notebook along with the provided links to external references should be sufficiently serve an informative document. A powerful laptop or mainstream desktop computer should be enough for you to execute most of the computationally inexpensive (e.g. inference with pretrained generators) cells.\n",
    "\n",
    "For the **interested practioner** keen on running the full notebook and playing around with different datasets and hyperparameters, a powerful desktop/server with at least one modern GPU is *highly* recommended, if not required. The median recommendation is 4-8 GPUs.\n",
    "\n",
    "If you do not have access to the recommended computational resources, it is possible (and fairly easy) to create a sufficiently powerful Virtual Machine (VM) on one of several cloud providers. We provide brief instructions and suggestions on how to get up and running with the following providers:\n",
    "\n",
    "- [Core Scientific] TODO\n",
    "- [Google Cloud] TODO\n",
    "- [IBM Cloud](IBM_Cloud.md)\n",
    "\n",
    "Nvidia and AWS also provide excellent offerings, but we are not able to provide specific instructions at this time.\n",
    "\n",
    "For the **hardcore \"GANologist\"** determined to reproduce (and exceed) the performance of the latest state-of-the-art GANs, we recommend using this notebook as a thorough introduction and reference before stepping up to full-fledged (officially unofficial) implementations such as [BigGAN-PyTorch](https://github.com/ajbrock/BigGAN-PyTorch).\n",
    "\n",
    "### System & Software Requirements\n",
    "Linux, MacOS, and Windows machines are supported, but we strongly recommend Linux for performance and compatibility reasons. \n",
    "\n",
    "- PyTorch, version 1.1 (stable)\n",
    "- tqdm, numpy, scipy, h5py\n",
    "\n",
    "### Recommendations\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "First, let's import the all of the  python packages we will use throughout the tutorial. In addition to standard import and PyTorch, we provide a small package called `ganocracy`, which aggregates a host of useful GAN specific functions and utilities in one place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,3\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# PYTORCH\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# GANOCRACY LIB\n",
    "sys.path.append('../')\n",
    "import ganocracy\n",
    "from ganocracy.data import datasets as dset\n",
    "from ganocracy.data import transforms\n",
    "from ganocracy.utils import visualizer as vutils\n",
    "from ganocracy import metrics\n",
    "\n",
    "# NOTEBOOK-SPECIFIC IMPORTS\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'\n",
    "print(os.environ['CUDA_VISIBLE_DEVICES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create a configuration dictionary `config`, which dictates how the entire notebook will run, and filled it with sensible defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    # --------------\n",
    "    # DATASET CONFIG\n",
    "    # --------------\n",
    "    'data_root': 'data',            # Root directory where datasets are stored.\n",
    "    'dataset': 'ImageNet',          # Name of dataset. Choices: See below.\n",
    "    'dataset_type': 'ImageHDF5',    # Type/format of dataset class. Choices: ['ImageFolder', 'ImageHDF5']\n",
    "    'resolution': 64,               # Image size (H, W) in pixels. Choices: [32, 64, 128, ...]\n",
    "\n",
    "    'split': 'train',               # Dataset split (train or val), if applicable.\n",
    "    'download': True,               # If data is not found, download and cache it.\n",
    "    'load_in_mem': True,            # Load entire dataset into RAM.\n",
    "    \n",
    "    # -----------------\n",
    "    # DATALOADER CONFIG\n",
    "    # -----------------\n",
    "    'shuffle': True,                # Shuffle training data\n",
    "    'batch_size': 512,              # Batch size per forward/backward pass.\n",
    "    'num_workers': 8,               # Number of workers used for data loading.\n",
    "\n",
    "    # ------------------\n",
    "    # MODEL ARCHITECTURE\n",
    "    # ------------------\n",
    "    'GAN_type': 'DCGAN',    # Type of GAN. Choices: ['DCGAN', 'SNGAN', 'SAGAN', 'BigGAN', 'BigGANDeep']\n",
    "    \n",
    "    'G_ch': 96,             # Channel multiplier - scales number of features per conv in G and D\n",
    "    'D_ch': 96,             # Typical choices: [64 (DCGAN, SN/SA-GAN), 96 (BigGAN), 128 (BigGANDeep)]\n",
    "    \n",
    "    'G_attn': 64,           # Resolution to insert self-attention (non-local) blocks.\n",
    "    'D_attn': 64,           # Does not apply (ignored) for DCGAN and SNGAN.\n",
    "    \n",
    "    'dim_z': 120,           # Dimension of latent z vector.\n",
    "    'hier': True,\n",
    "    'shared_dim': 128,\n",
    "    \n",
    "    'G_init': 'ortho',      # Weight initialization method.\n",
    "    'D_init': 'ortho',      # Choices: ['normal', 'xavier', 'ortho'].\n",
    "    \n",
    "    'G_n1': 'inplace_relu', # Activation function.\n",
    "    'D_n1': 'inplace_relu', # Choices: ['relu', 'inplace_relu']\n",
    "    'G_eval_mode': True,\n",
    "    'ngpu': 1,\n",
    "  \n",
    "    # ---------------\n",
    "    # TRAINING CONFIG\n",
    "    # ---------------\n",
    "    'num_epochs': 100,        # Number of training epochs (1 epoch = 1 presentation of full dataset).\n",
    "    'num_D_steps': 1,         # Number of updates to Discriminator per 1 step of Generator.\n",
    "    \n",
    "    'num_G_accumulations': 4, # Number of gradient accumulations per step of G and D.\n",
    "    'num_D_accumulations': 4, # Technique to spoof larger batch sizes.\n",
    "    \n",
    "    'loss_type': 'BCE',       # Loss function type. Choices ['BCE', 'hinge']\n",
    "    \n",
    "    # ----------------\n",
    "    # OPTIMIZER CONFIG\n",
    "    # ----------------\n",
    "    'G_lr': 1e-4,            # Learning rates for G and D.\n",
    "    'D_lr': 4e-4,            # Should be same \n",
    "    \n",
    "    'G_betas': (0.5, 0.999), # Betas for Adam optimizers\n",
    "    'D_betas': (0.5, 0.999),\n",
    "    \n",
    "    'ema': True,\n",
    "    'use_ema': True,\n",
    "    'ema_start': 20000,\n",
    "    'test_every': 2000,\n",
    "    'save_every': 1000,\n",
    "    'num_best_copies': 5,\n",
    "    'num_save_copies': 2,\n",
    "    'seed': 0,\n",
    "    'use_multiepoch_sampler': True,\n",
    "    \n",
    "    'TORCH_HOME': None,  # Path to where pretrained models are saved. Default: None => $HOME/.torch\n",
    "}\n",
    "\n",
    "config.update({\n",
    "    # Update with standard number of classes\n",
    "    # based on choice of dataset.\n",
    "    'num_classes': {\n",
    "        'celeba': 1,\n",
    "        'CIFAR10': 10,\n",
    "        'CIFAR100': 100,\n",
    "        'ImageNet': 1000,\n",
    "        'Places365': 365,\n",
    "    }.get(config['dataset'], 1),\n",
    "    \n",
    "    # Prepare data; the Discriminator's batch size is all that needs to be passed\n",
    "    # to the dataloader, as G doesn't require dataloading.\n",
    "    # Note that at every loader iteration we pass in enough data to complete\n",
    "    # a full D iteration (regardless of number of D steps and accumulations)\n",
    "    'D_batch_size': config['batch_size'] * config['num_D_steps'] * config['num_D_accumulations'],\n",
    "    \n",
    "    # Determine whether or not GPUs are available.\n",
    "    'device': torch.device(\"cuda:0\" if (torch.cuda.is_available() and config['ngpu'] > 0) else \"cpu\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In this tutorial, you are free to choose from a range of popular datasets ranging in difficulty and diversity. These include:  \n",
    "- CelebA: \n",
    "- Cifar10/100\n",
    "- [TODO include LSUN?]\n",
    "- ImageNet\n",
    "- Places365\n",
    "\n",
    "By default, we will use the [Places365 dataset](http://places2.csail.mit.edu/) which contains more than 10 million images comprising 400+ unique scene categories. You can manually download the full dataset [here](http://places2.csail.mit.edu/download.html), although we provide standard PyTorch dataloaders that download, cache and preprocess several standard datasets as part of this tutorial. Therefore, preparing a dataloader can be accomplished with just a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(name, root_dir=None, resolution=128, filetype='tar'):\n",
    "    if filetype == 'tar':\n",
    "        url = dset.data_urls[name]['tar']\n",
    "        data_dir = dset.load_data_from_url(url, root_dir)\n",
    "        dataset = dset.ImageFolder(root=data_dir,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.CenterCropLongEdge(),\n",
    "                                       transforms.Resize(resolution),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                            (0.5, 0.5, 0.5))\n",
    "                                    ]))\n",
    "    elif filetype == 'hdf5':\n",
    "        url = dset.data_urls[name]['hdf5'][resolution]\n",
    "        hdf5_file = dset.load_data_from_url(url, root_dir)\n",
    "        dataset = dset.ImageHDF5(hdf5_file)\n",
    "    else:\n",
    "        raise ValueError('Unreconized filetype: {}'.format(filetype))\n",
    "    return dataset\n",
    "\n",
    "# dataset = get_dataset(config['dataset'],\n",
    "#                       root_dir=config['data_root'],\n",
    "#                       resolution=config['resolution'])\n",
    "\n",
    "# dataloader = torch.utils.data.DataLoader(dataset,\n",
    "#                                          shuffle=config['shuffle'],\n",
    "#                                          batch_size=config['batch_size'],\n",
    "#                                          num_workers=config['num_workers'])\n",
    "# vutils.visualize_data(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resolution = config['resolution']\n",
    "# dataset = torchvision.datasets.ImageNet(config['data_root'],\n",
    "#                                         split=config['split'],\n",
    "#                                         download=config['download'],\n",
    "#                                         transform=transforms.Compose([\n",
    "#                                             transforms.CenterCropLongEdge(),\n",
    "#                                             transforms.Resize(resolution),\n",
    "#                                             transforms.ToTensor(),\n",
    "#                                             transforms.Normalize((0.5, 0.5, 0.5),\n",
    "#                                                                  (0.5, 0.5, 0.5))\n",
    "#                                     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet-64.hdf5\n",
      "data/ImageNet-64.hdf5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-19a44099cc99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mhdf5_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_root'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdf5_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mhdf5_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_root'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdf5_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageHDF5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_in_mem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         dataloader = torch.utils.data.DataLoader(dataset,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# # We can use an image folder dataset the way we have it setup.\n",
    "# # Create the dataset\n",
    "if config['dataset'] in ['CIFAR10', 'CIFAR100']:\n",
    "    image_size = 32\n",
    "    CIFAR = getattr(torchvision.datasets, config['dataset'])\n",
    "    dataset = CIFAR(root=config['data_root'], train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                    ]))\n",
    "elif config['dataset'] in ['CelebA']:\n",
    "    CelebA = getattr(torchvision.datasets, config['dataset'])\n",
    "    dataset = CelebA(root=config['data_root'],  download=config['download'],\n",
    "                     transform=transforms.Compose([\n",
    "                         transforms.Resize(config['resolution']),\n",
    "                         transforms.CenterCrop(config['resolution']),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                     ]))\n",
    "else:\n",
    "    if config['dataset_type'] == 'ImageFolder':\n",
    "        dataset = dset.ImageFolder(root=config['data_root'],\n",
    "                                   download=config['download'],\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.Resize(config['resolution']),\n",
    "                                       transforms.CenterCrop(config['resolution']),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "    elif config['dataset_type'] == 'ImageHDF5':\n",
    "        hdf5_name = '{}-{}.hdf5'.format(config['dataset'], config['resolution'])\n",
    "        print(hdf5_name)\n",
    "        hdf5_file = os.path.join(config['data_root'], hdf5_name)\n",
    "        print(hdf5_file)\n",
    "        hdf5_file = dset.make_hdf5(dataloader, config['data_root'], hdf5_name)\n",
    "        dataset = dset.ImageHDF5(hdf5_file, load_in_mem=True)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=D_batch_size,\n",
    "                                             num_workers=workers, \n",
    "                                             drop_last=True)\n",
    "\n",
    "# # Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'],\n",
    "                                         shuffle=config['shuffle'], num_workers=config['num_workers'])\n",
    "# gvutils.visualize_data(dataloader)\n",
    "# # Decide which device we want to run on\n",
    "device = config['device']\n",
    "\n",
    "# # Plot some training images\n",
    "# real_batch = next(iter(dataloader))\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Training Images\")\n",
    "# plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_name = '{}-{}.hdf5'.format(config['dataset'], config['resolution'])\n",
    "print(hdf5_name)\n",
    "hdf5_file = os.path.join(config['data_root'], hdf5_name)\n",
    "print(hdf5_file)\n",
    "hdf5_file = dset.make_hdf5(dataloader, config['data_root'], hdf5_name)\n",
    "dataset = dset.ImageHDF5(hdf5_file, load_in_mem=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                     shuffle=config['shuffle'],\n",
    "                                     batch_size=config['D_batch_size'],\n",
    "                                     num_workers=config['num_workers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your own data\n",
    "\n",
    "If you would like to train a GAN on a your own custom dataset, subclassing [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) is a sensible approach as it allows you to make use of PyTorch's dataloading utilities, including the multi-threaded DataLoader and transforms from above.\n",
    "\n",
    "If your training data consist of image files, the [`torchvision.datasets.ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) class facilitates easy dataloading. Simply arrange your files in the following way:\n",
    "\n",
    "    root/dogball/xxx.png\n",
    "    root/dogball/xxy.png\n",
    "    root/dogball/xxz.png\n",
    "\n",
    "    root/cat/123.png\n",
    "    root/cat/nsdf3.png\n",
    "    root/cat/asd932_.png\n",
    "    \n",
    "where each subdirectory of `root` is considered an image category containing examples of that category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data as single HDF5 file (optional, for additional performance)\n",
    "\n",
    "If your dataset consists of large, high-resolution images, then repeatedly applying transforms to the raw images (cropping, resizing) results in many wasted CPU cycles. Furthermore, increasing your batchsize puts additional I/O strain on your filesystem. Together, these factors may produce a dataloading bottleneck where your GPUs consume data faster than your system can produce it. To remedy this, you may choose to prepare a pre-processed HDF5 version of your target dataset using the utilities provided. Moreover, if I/O still appears to be the bottleneck, you may choose to load the entire dataset into RAM by setting `load_in_mem` dataset kwargs to `True` (if your system can support this).\n",
    "\n",
    "Another advantage to using a single, preprocessed HDF5 is the ease with which you can transfer data to multiple remote machines, which normally becomes cumbersome and time consuming when attempting large, distributed experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet-64.hdf5\n",
      "data/ImageNet-64.hdf5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-171d993ad8fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhdf5_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_root'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdf5_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhdf5_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_root'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdf5_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageHDF5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_in_mem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'load_in_mem'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m dataloader = torch.utils.data.DataLoader(dataset,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "hdf5_name = '{}-{}.hdf5'.format(config['dataset'], config['resolution'])\n",
    "print(hdf5_name)\n",
    "hdf5_file = os.path.join(config['data_root'], hdf5_name)\n",
    "print(hdf5_file)\n",
    "hdf5_file = dset.make_hdf5(dataloader, config['data_root'], hdf5_name)\n",
    "dataset = dset.ImageHDF5(hdf5_file, load_in_mem=config['load_in_mem'])\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         shuffle=config['shuffle'],\n",
    "                                         batch_size=config['D_batch_size'],\n",
    "                                         num_workers=config['num_workers'])\n",
    "vutils.visualize_data(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to measure sample quality during training\n",
    "\n",
    "Typically, when training any sort of neural network, it is standard practice to monitor the value of the objective function (loss) throughout the course of the experiment. With GANs, it is also common to break the loss into individual components.\n",
    "\n",
    "Objectively evaluating implicit generative models is difficult (Theis et al., 2015). A variety of works\n",
    "have proposed heuristics for measuring the sample quality of models without tractable likelihoods\n",
    "(Salimans et al., 2016; Heusel et al., 2017; Binkowski et al., 2018; Wu et al., 2017). Of these, ´\n",
    "the Inception Score (IS, Salimans et al. (2016)) and Frechet Inception Distance (FID, Heusel et al. ´\n",
    "(2017)) have become popular despite their notable flaws (Barratt & Sharma, 2018). We employ\n",
    "them as approximate measures of sample quality, and to enable comparison against previous work.\n",
    "\n",
    "While training, it would be very useful to objectively measure the quality of generated samples in order to understand how training is progressing (without having to manually assign subjective ratings to a large. Moreover,  To this end, researchers have developed metrics which seek to capture the quality of the generated samples, with the two most popular being:\n",
    "- Inception Score (IS)\n",
    "- Frechet Distance (FID)\n",
    "\n",
    "\n",
    "Higher IS values mean better image quality, but not necessarily diversity.\n",
    "Lower FID values mean better image quality and diversity.\n",
    "\n",
    "or datasets other than ImageNet, Inception Score can be a very poor measure of quality, so you will likely want to use --which_best FID instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inception_moments_file = metrics.calculate_inception_moments(dataloader, config['data_root'], config['dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "\n",
    "## Convolutional GANS \n",
    "\n",
    "[Deep Convolutional GANS](https://arxiv.org/abs/1511.06434) (DCGANs) represented a major step forward in the sucess of GAN image synthesis. The introduction of convolutional layers, which had already proven successful for disciminative computer vision tasks\n",
    "Structure well suited for image generatation.\n",
    "When applied to images, G and D are usually convolutional neural networks (Radford et al., 2016).\n",
    "![DCGAN Generator Architecure](assets/dcgan_generator.png)\n",
    "\n",
    "Almost all GANs used for image synthesis now follow some variation of DCGAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator consists of GBlocks, which progressively increase the spatial dimensions while decreasing the feature volume depth.\n",
    "\n",
    "Batch norm follows the transposed Convolution, which works to combat poor initialization schemes and mode collapes. *Recall, batch norm works by normalizing the inputs features to have zero mean and unit variance.*\n",
    "\n",
    "*Mode Collapse*: When the generator falls into a situation where it produces an extremely limited set output patterns (\"modes\") despite maintaining diversity on input noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# DCGAN Generator\n",
    "####################################################################\n",
    "\n",
    "class GBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels,\n",
    "                                       kernel_size=4, stride=2,\n",
    "                                       padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward method of GBlock.\n",
    "        \n",
    "        This block increases the spatial resolution by 2:\n",
    "        \n",
    "            input:  [batch_size, in_channels, H, W]\n",
    "            output: [batch_size, out_channels, 2*H, 2*W]\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.conv(input)\n",
    "        x = self.bn(x)\n",
    "        out = self.act(x)\n",
    "        return out\n",
    "\n",
    "class DCGANGenerator(nn.Module):\n",
    "    \"\"\"DCGAN Generator.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Maps output resoluton to number of GBlocks.\n",
    "    res2blocks = {\n",
    "        32: 3,\n",
    "        64: 4,\n",
    "        128: 5,\n",
    "        256: 6,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, dim_z=128, resolution=128, G_ch=64, \n",
    "                block=GBlock):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.G_ch = G_ch\n",
    "        self.dim_z = dim_z\n",
    "\n",
    "        self.num_blocks = self.res2blocks[resolution]\n",
    "        self.ch_nums = [2**i for i in range(self.num_blocks)]\n",
    "        self.ch_nums += self.ch_nums[-1:]\n",
    "        self.ch_nums = list(reversed(self.ch_nums))\n",
    "        \n",
    "        self.linear = nn.Linear(dim_z, G_ch * self.ch_nums[0] * 4**2)\n",
    "        self.GBlocks = nn.Sequential(*[\n",
    "            block(G_ch * in_c, G_ch * out_c)\n",
    "            for in_c, out_c in zip(self.ch_nums, self.ch_nums[1:])\n",
    "        ])\n",
    "        self.out = nn.Conv2d(G_ch * 1, 3, 3, padding=1)  # RGB image has 3 channels\n",
    "        self.tanh = nn.Tanh()                            # \"Squashes\" out to be in range[-1, 1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(x.size(0), -1, 4, 4)\n",
    "        x = self.GBlocks(x)\n",
    "        return self.tanh(self.out(x))\n",
    "    \n",
    "dim_z = 128\n",
    "z = torch.rand(10, config['dim_z'])\n",
    "G = DCGANGenerator(dim_z=config['dim_z'], resolution=config['resolution'])\n",
    "print(G)\n",
    "print('output shape:', G(z).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBlocks, found in the discriminator, are near perfect inverses to GBlocks, trading spatial dimension for feature depth, with the exception of using LearkyReLUs instead of ReLUs for their nonlinearities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# DCGAN Discriminator\n",
    "####################################################################\n",
    "\n",
    "class DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward method of DBlock.\n",
    "        \n",
    "        This block decreases the spatial resolution by 2:\n",
    "        \n",
    "            input:  [batch_size, in_channels, H, W]\n",
    "            output: [batch_size, out_channels, H/2, W/2]\n",
    "        \"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class DCGANDiscriminator(nn.Module):\n",
    "    \"\"\"DCGAN discriminator.\"\"\"\n",
    "    \n",
    "    # Maps output resoluton to number of DBlocks.\n",
    "    res2blocks = {\n",
    "        32: 3,\n",
    "        64: 4,\n",
    "        128: 5,\n",
    "        256: 6,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, resolution=128, D_ch=64, block=DBlock):\n",
    "        super().__init__()\n",
    "        self.D_ch = D_ch\n",
    "        self.num_blocks = self.res2blocks[resolution]\n",
    "        self.ch_nums = [2**i for i in range(self.num_blocks)]\n",
    "        self.input_layer = nn.Conv2d(3, D_ch, 3, padding=1)\n",
    "        \n",
    "        self.DBlocks = nn.Sequential(*[\n",
    "            block(D_ch * in_c, D_ch * out_c)\n",
    "            for in_c, out_c in zip(self.ch_nums, self.ch_nums[1:])\n",
    "        ])\n",
    "        \n",
    "        self.out = nn.Conv2d(D_ch * self.ch_nums[-1], 1, 3, 1, 0)\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.DBlocks(x)\n",
    "        x = self.act(torch.mean(self.out(x), [2, 3]))\n",
    "        return x\n",
    "\n",
    "    \n",
    "x = torch.rand(10, 3, config['resolution'], config['resolution'])\n",
    "D = Discriminator(resolution=config['resolution'])\n",
    "print(D)\n",
    "print('output shape:', D(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GANs (cGANs)\n",
    "\n",
    "Conditional GANs are an interesting extension to the GAN framework, allowing for much greater control over the final output of the generator. During training, both the generator and discriminator are conditioned on some additional data $y$, most commonly an image class label. The progression of approaches to conditional GAN discriminators is nicely summarized in the following figure:\n",
    "\n",
    "![Approaches to conditional GANs](assets/cGANs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cGANDiscriminator(Discriminator):\n",
    "    \"\"\"cGAN discriminator.\"\"\"\n",
    "    \n",
    "    # Maps output resoluton to number of DBlocks.\n",
    "    res2blocks = {\n",
    "        32: 3,\n",
    "        64: 4,\n",
    "        128: 5,\n",
    "        256: 6,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, num_classes=1000, resolution=128, D_ch=64, block=DBlock):\n",
    "        super().__init__(resolution=resolution, D_ch=D_ch, block=block)\n",
    "        \n",
    "        self.embed = nn.Embedding(num_classes, self.ch_nums[-1])\n",
    "        self.linear = nn.Linear(self.ch_nums[-1], 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        h = self.input_layer(h)\n",
    "        h = self.DBlocks(h)\n",
    "        h = self.out(h)\n",
    "        \n",
    "        # Apply global sum pooling\n",
    "        h = torch.sum(self.act(h), [2, 3])\n",
    "        \n",
    "        # Initial class-unconditional output.\n",
    "        out = self.linear(h)  \n",
    "        \n",
    "        # Get projection of final featureset onto class vectors and add to evidence\n",
    "        out = out + torch.sum(self.embed(y) * h, 1, keepdim=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConditionalBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, eps=1e-4, momentum=0.1,\n",
    "                 linear_func=SNLinear):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.bn = nn.BatchNorm2d(num_features, affine=False, eps=eps, momentum=momentum)\n",
    "        self.gamma_embed = linear_func(num_classes, num_features, bias=False)\n",
    "        self.beta_embed = linear_func(num_classes, num_features, bias=False)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # First, compute standard batchnorm stats.\n",
    "        out = self.bn(x)\n",
    "        \n",
    "        # Learn class specific scale and shift.\n",
    "        gamma = self.gamma_embed(y) + 1\n",
    "        beta = self.beta_embed(y)\n",
    "        out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Normalization (SNGANs)\n",
    "\n",
    "\n",
    "**Problem**: Adversarial training is a highly dynamic characterized by instability, particularly in the Discriminator network.\n",
    "\n",
    "Miyato et al proposed a normalization technique called [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957) (SNGANs), which enforces Lipschitz continuity on the weights of the discriminator and stabilizes training.\n",
    "\n",
    "Briefly, Lipschitz continuity relates to how quickly a function can change. For example, every function that has bounded first derivatives is Lipschitz, and the smallest such bound is called the Lipschitz constant. \n",
    "\n",
    "Spectral normed layers enforce Lipschitz continuity by normalizing its parameters with running estimates of their first singular values. These estimates can be computed efficiently using the power iteration method described below:\n",
    "\n",
    "![spectral normalization algorithm](assets/spectral_norm_algorithm.png)\n",
    "\n",
    "Sn was originally shown to be useful in D, but later also shown to improve stability in G, allowing for fewer D steps per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def power_iteration(W, u_, update=True, eps=1e-12):\n",
    "    \"\"\"Apply num_itrs steps of the power method to estimate top N singular values.\"\"\"\n",
    "    # Lists holding singular vectors and values\n",
    "    us, vs, svs = [], [], []\n",
    "    for i, u in enumerate(u_):\n",
    "        # Run one step of the power iteration.\n",
    "        with torch.no_grad():\n",
    "            v = torch.matmul(u, W)\n",
    "            # Run Gram-Schmidt to subtract components of all other singular vectors.\n",
    "            v = F.normalize(gram_schmidt(v, vs), eps=eps)\n",
    "            # Add to the list\n",
    "            vs += [v]\n",
    "            # Update the other singular vector\n",
    "            u = torch.matmul(v, W.t())\n",
    "            # Run Gram-Schmidt to subtract components of all other singular vectors.\n",
    "            u = F.normalize(gram_schmidt(u, us), eps=eps)\n",
    "            # Add to the list\n",
    "            us += [u]\n",
    "            if update:\n",
    "                u_[i][:] = u\n",
    "        # Compute this singular value and add it to the list\n",
    "        svs += [torch.squeeze(torch.matmul(torch.matmul(v, W.t()), u.t()))]\n",
    "        # svs += [torch.sum(F.linear(u, W.transpose(0, 1)) * v)]\n",
    "    return svs, us, vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Mechanism (Diversity)\n",
    "\n",
    "First proposed by the GANfather, Ian Goodfellow himself, in the paper [Self-Attention Generative Adversarial Networks](https://arxiv.org/abs/1805.08318)(SA-GANs), the introduction of a self-attention mechanism to GANs is aimed at modeling long-range dependenices in a computationally efficient manner.\n",
    "\n",
    "### Observation and Inspiration\n",
    "Prior to SA-GANs, researchers noticed that while vanilla DCGAN-style GANs continued to improve on datasets with limited number of classes (such as faces), they still stuggled to learn the image distributions of diverse multi-class datasets like Imagenet. \n",
    "\n",
    "convolutions are (spatially) local operations, with the extend of spatial dependencies (receptive field) limited by the kernel size. Thus, capturing long-range (non-local) dependenices is not really possible since the outputs of a convolution at one spatial location have no effect on the outputs at another distant location.\n",
    "\n",
    "Possible solutions:\n",
    "\n",
    "- increase kernel size (slow and inefficient)\n",
    "- deeper network (more layers) so that later layers have larger receptive fields (more parameters, more difficult to train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Self-Attention GAN regions attended](assets/sagan_regions.png)\n",
    "\n",
    "\n",
    "![Self-Attention Block](assets/sa_block.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"A non-local block as used in SA-GAN.\n",
    "    \n",
    "    NOTE: The implementation as described in the paper is largely incorrect;\n",
    "    refer to the released code for the actual implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ch, conv_func=nn.Conv2d):\n",
    "        super().__init__()\n",
    "        # Channel multiplier\n",
    "        self.ch = ch\n",
    "        self.conv_func = conv_func\n",
    "        self.theta = self.conv_func(self.ch, self.ch // 8, kernel_size=1, padding=0, bias=False)\n",
    "        self.phi = self.conv_func(self.ch, self.ch // 8, kernel_size=1, padding=0, bias=False)\n",
    "        self.g = self.conv_func(self.ch, self.ch // 2, kernel_size=1, padding=0, bias=False)\n",
    "        self.o = self.conv_func(self.ch // 2, self.ch, kernel_size=1, padding=0, bias=False)\n",
    "        \n",
    "        # Learnable gain parameter\n",
    "        self.gamma = P(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        # Apply convs\n",
    "        theta = self.theta(x)\n",
    "        phi = F.max_pool2d(self.phi(x), [2,2])\n",
    "        g = F.max_pool2d(self.g(x), [2,2])\n",
    "        \n",
    "        # Perform reshapes\n",
    "        theta = theta.view(-1, self. ch // 8, x.shape[2] * x.shape[3])\n",
    "        phi = phi.view(-1, self. ch // 8, x.shape[2] * x.shape[3] // 4)\n",
    "        g = g.view(-1, self. ch // 2, x.shape[2] * x.shape[3] // 4)\n",
    "        \n",
    "        # Matmul and softmax to get attention maps\n",
    "        beta = F.softmax(torch.bmm(theta.transpose(1, 2), phi), -1)\n",
    "        \n",
    "        # Attention map times g path\n",
    "        o = self.o(torch.bmm(g, beta.transpose(1,2)).view(-1, self.ch // 2, x.shape[2], x.shape[3]))\n",
    "        return self.gamma * o + x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scaling Up GANs\n",
    "\n",
    "### Batch Size\n",
    "Simply increasing the batch size by a factor of 8 improves the state-of-the-art IS by 46%. We conjecture that this is a result of each batch covering more modes, providing better gradients for both networks.\n",
    "\n",
    "### Number of channels\n",
    "We then increase the width (number of channels) in each layer by 50%, approximately doubling the\n",
    "number of parameters in both models. This leads to a further IS improvement of 21%, which we\n",
    "posit is due to the increased capacity of the model relative to the complexity of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two-timescale update rule (TTUR)\n",
    "\n",
    "## Exponential Moving Average (EMA)\n",
    "We use an exponential moving average of the weights of G at sampling time, with a decay rate set to\n",
    "0.9999\n",
    "\n",
    "## Gradient Accumulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "![BigGAN Architecture](assets/biggan_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGANGenerator(nn.Module):\n",
    "    \"\"\"DCGAN Generator.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Maps output resoluton to number of GBlocks.\n",
    "    res2blocks = {\n",
    "        32: 3,\n",
    "        64: 4,\n",
    "        128: 5,\n",
    "        256: 6,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, dim_z=128, resolution=128, G_ch=64, \n",
    "                block=GBlock):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.G_ch = G_ch\n",
    "        self.dim_z = dim_z\n",
    "\n",
    "        self.num_blocks = self.res2blocks[resolution]\n",
    "        self.ch_nums = [2**i for i in range(self.num_blocks)]\n",
    "        self.ch_nums += self.ch_nums[-1:]\n",
    "        self.ch_nums = list(reversed(self.ch_nums))\n",
    "        \n",
    "        self.linear = nn.Linear(dim_z, G_ch * self.ch_nums[0] * 4**2)\n",
    "        self.GBlocks = nn.Sequential(*[\n",
    "            block(G_ch * in_c, G_ch * out_c)\n",
    "            for in_c, out_c in zip(self.ch_nums, self.ch_nums[1:])\n",
    "        ])\n",
    "        self.out = nn.Conv2d(G_ch * 1, 3, 3, padding=1)  # RGB image has 3 channels\n",
    "        self.tanh = nn.Tanh()                            # \"Squashes\" out to be in range[-1, 1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(x.size(0), -1, 4, 4)\n",
    "        x = self.GBlocks(x)\n",
    "        return self.tanh(self.out(x))\n",
    "    \n",
    "dim_z = 128\n",
    "z = torch.rand(10, config['dim_z'])\n",
    "G = DCGANGenerator(dim_z=config['dim_z'], resolution=config['resolution'])\n",
    "print(G)\n",
    "print('output shape:', G(z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class GBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_features):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
    "        self.bn = ConditionalBatchNorm2d(out_channels, num_features)\n",
    "        self.act = nn.ReLU(True)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x, y)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    res2blocks = {\n",
    "        32: 3,\n",
    "        64: 4,\n",
    "        128: 5,\n",
    "        256: 6,\n",
    "    }\n",
    "    def __init__(self, dim_z=128, num_classes=2, resolution=128, G_ch=64, class_dim=128):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.G_ch = G_ch\n",
    "        self.dim_z = dim_z\n",
    "        self.class_dim = class_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        \n",
    "        self.num_blocks = self.res2blocks[resolution]\n",
    "        self.fnums = [2**i for i in range(self.num_blocks)]\n",
    "        self.fnums += self.fnums[-1:]\n",
    "        self.fnums = list(reversed(self.fnums))\n",
    "        \n",
    "        self.class_linear = nn.Linear(num_classes, class_dim)\n",
    "        self.linear = nn.Linear(dim_z, G_ch * self.fnums[0] * 4**2)\n",
    "        \n",
    "        self.GBlocks = nn.ModuleList ([\n",
    "            GBlock(G_ch * in_c, G_ch * out_c, class_dim)\n",
    "            for in_c, out_c in zip(self.fnums, self.fnums[1:])])\n",
    "\n",
    "        self.out = nn.Conv2d(G_ch * 1, 3, 3, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, z, y):\n",
    "        class_embed = self.shared(y)\n",
    "        return self.generate(z, class_embed)\n",
    "\n",
    "    \n",
    "    def shared(self, y):\n",
    "        y = self.onehot(y, self.num_classes)\n",
    "        return self.class_linear(y)\n",
    "    \n",
    "    def generate(self, z, class_embed):\n",
    "        z = self.linear(z).view(z.size(0), -1, 4, 4)\n",
    "        for block in self.GBlocks:\n",
    "            z = block(z, class_embed)\n",
    "        return self.tanh(self.out(z))\n",
    "    \n",
    "    def onehot(self, y, num_classes):\n",
    "        \"\"\"Transform int labels to onehot tensor, if needed.\n",
    "        \n",
    "        Args:\n",
    "            y (torch.Tensor): Class labels (either ints or onehot).\n",
    "                size: [batch_size,] or [batch_size, num_classes]\n",
    "            num_classes: Total number of classes.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: onehot representation of class targets.\n",
    "        \n",
    "        \"\"\"\n",
    "        y = y.squeeze()\n",
    "        if y.dim() == 1:\n",
    "            y = y.unsqueeze(-1)\n",
    "            y = torch.zeros((y.size(0), num_classes),\n",
    "                            device=y.device).scatter(1, y.long(), 1)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "\n",
    "dim_z = 128\n",
    "z = torch.rand(4, dim_z)\n",
    "y = torch.ones(4) * 1\n",
    "G = Generator(dim_z=dim_z)   \n",
    "print(G)\n",
    "print(G(z, y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the generator\n",
    "netG = Generator(dim_z=config['dim_z'],\n",
    "                 num_classes=config['num_classes'],\n",
    "                 resolution=config['resolution']).to(config['device'])\n",
    "\n",
    "# Create the Discriminator\n",
    "netD = Discriminator(resolution=config['resolution']).to(config['device'])\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (config['device'].type == 'cuda') and (config['ngpu'] > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(config['ngpu'])))\n",
    "    netD = nn.DataParallel(netD, list(range(config['ngpu'])))\n",
    "    \n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "# netG.apply(weights_init)\n",
    "# netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.G = DCGANGenerator(dim_z=config['dim_z'],\n",
    "                                resolution=config['resolution'],\n",
    "                                G_ch=config['G_ch'])\n",
    "        self.D = Discriminator(resolution=config['resolution'], D_ch=config['D_ch'])\n",
    "        self.G.optim = optim.Adam(self.G.parameters(), lr=config['G_lr'], betas=config['G_betas'])\n",
    "        self.D.optim = optim.Adam(self.D.parameters(), lr=config['D_lr'], betas=config['D_betas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if config['loss_type'] == 'BCE':\n",
    "    criterion_D = nn.BCELoss()\n",
    "    criterion_G = nn.BCELoss()\n",
    "elif config['loss_t']\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "netD.optim = optim.Adam(netD.parameters(), lr=config['D_lr'], betas=config['G_betas'])\n",
    "netG.optim = optim.Adam(netG.parameters(), lr=config['G_lr'], betas=config['D_betas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "if TRAIN:\n",
    "# Training Loop\n",
    "\n",
    "    print(\"Starting Training Loop...\")\n",
    "    # For each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # For each batch in the dataloader\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            \n",
    "            counter = 0\n",
    "            data = [d.to(device) for d in data]\n",
    "            x, y = [torch.split(d, batch_size) for d in data]\n",
    "            \n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "\n",
    "            netD.zero_grad()\n",
    "            real_labels = torch.full((x[-1].size(0),), real_label, device=device)\n",
    "            fake_labels = torch.full((x[-1].size(0),), fake_label, device=device)\n",
    "            \n",
    "            \n",
    "            for step_index in range(num_D_steps):\n",
    "                netD.zero_grad()\n",
    "                for accumulation_index in range(num_D_accumulations):\n",
    "                    \n",
    "                    ################## ** Train with all-real batch ** ##\n",
    "\n",
    "                    # Forward pass real batch through D\n",
    "                    output = netD(x[counter]).view(-1)\n",
    "                    \n",
    "                    # Calculate loss on all-real batch\n",
    "                    D_loss_real = criterion(output, real_labels[:output.size(0)]) / float(num_D_accumulations)\n",
    "                    D_loss_real.backward()\n",
    "                    D_x = output.mean().item()\n",
    "\n",
    "                    ## ** Train with all-fake batch ** ##\n",
    "                \n",
    "                    # Generate batch of latent vectors and targets\n",
    "                    noise = torch.randn(batch_size, nz, device=device)\n",
    "                    targets = torch.randint(num_classes, (batch_size, 1), device=device)\n",
    "                    # Generate fake image batch with G\n",
    "                    fake_image = netG(noise, targets)\n",
    "\n",
    "                    # Classify all fake batch with D\n",
    "                    output = netD(fake_image.detach()).view(-1)\n",
    "                    \n",
    "                    # Calculate D's loss on the all-fake batch\n",
    "                    D_loss_fake = criterion(output, fake_labels) / float(num_D_accumulations)\n",
    "                    D_loss_fake.backward()\n",
    "                    D_G_z1 = output.mean().item()\n",
    "                    \n",
    "                    # Add the gradients from the all-real and all-fake batches\n",
    "#                     D_loss = (D_loss_real + D_loss_fake) / float(num_D_accumulations)\n",
    "                    D_loss = (D_loss_real + D_loss_fake)\n",
    "                    counter += 1\n",
    "                    \n",
    "                # Update D\n",
    "                optimizerD.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            \n",
    "            netG.zero_grad()\n",
    "            for accumulation_index in range(num_G_accumulations):\n",
    "                \n",
    "                # Generate batch of latent vectors and targets\n",
    "                noise = torch.randn(batch_size, nz, device=device)\n",
    "                targets = torch.randint(num_classes, (batch_size, 1), device=device)\n",
    "                fake_image = netG(noise, targets)\n",
    "                output = netD(fake_image).view(-1)\n",
    "                # Calculate G's loss based on this output\n",
    "                G_loss = criterion(output, real_labels) / float(num_G_accumulations)\n",
    "                    \n",
    "                # Calculate gradients for G\n",
    "                G_loss.backward()\n",
    "                D_G_z2 = output.mean().item()\n",
    "                \n",
    "            # Update G\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Output training stats\n",
    "            if i % 10 == 0:\n",
    "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                      % (epoch, num_epochs, i, len(dataloader),\n",
    "                         D_loss.item(), G_loss.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(G_loss.item())\n",
    "            D_losses.append(D_loss.item())\n",
    "\n",
    "            # Check how the generator is doing by saving G's output on fixed_noise\n",
    "            if (iters % 100 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "                with torch.no_grad():\n",
    "                    fake_image = netG(fixed_noise, fixed_labels).detach().cpu()\n",
    "                fname = os.path.join(samples_dir, model_name, f'{iters}.jpg')\n",
    "                vutils.save_image(fake_image, fname, padding=2, normalize=True )\n",
    "                img_list.append(vutils.make_grid(fake_image, padding=2, normalize=True))\n",
    "\n",
    "            iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gan = DCGAN(config)\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "    \n",
    "        \n",
    "        gan.D.zero_grad()\n",
    "        real_labels = torch.full((config['batch_size'],), 1, device=config['device'])\n",
    "        \n",
    "        D_real = gan.D(x)\n",
    "        D_loss_real = criterion(D_real, real_labels)\n",
    "        \n",
    "        z = torch.randn(config['batch_size'], config['dim_z'], device=config['device'])\n",
    "        gy = torch.LongTensor(config['batch_size'], 1).random_() % config['num_classes']\n",
    "        G_z = gan.G(z, gy)\n",
    "        D_fake = gan.D(G_z, gy)\n",
    "        D_loss_fake = criterion(D_fake, real_labels-1)\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        D_loss.backward()\n",
    "        gan.D.optim.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAIN TRAINING FUNCTION\n",
    "def GAN_training_function(G, D, GD, z_, y_, ema, state_dict, config):\n",
    "    def train(x, y):\n",
    "        G.optim.zero_grad()\n",
    "        D.optim.zero_grad()\n",
    "        # How many chunks to split x and y into?\n",
    "        x = torch.split(x, config['batch_size'])\n",
    "        y = torch.split(y, config['batch_size'])\n",
    "        counter = 0\n",
    "\n",
    "        # Optionally toggle D and G's \"require_grad\"\n",
    "        if config['toggle_grads']:\n",
    "            utils.toggle_grad(D, True)\n",
    "            utils.toggle_grad(G, False)\n",
    "\n",
    "        ###############################################################\n",
    "        #                    TRAIN DISCRIMINATOR\n",
    "        ###############################################################\n",
    "        for step_index in range(config['num_D_steps']):\n",
    "            # If accumulating gradients, loop multiple times before an optimizer step\n",
    "            for accumulation_index in range(config['num_D_accumulations']):\n",
    "                z_.sample_(), y_.sample_()\n",
    "                D_fake, D_real = GD(z_[:config['batch_size']], y_[:config['batch_size']],\n",
    "                                    x[counter], y[counter], train_G=False,\n",
    "                                    split_D=config['split_D'])\n",
    "\n",
    "                # Compute components of D's loss, average them, and divide by\n",
    "                # the number of gradient accumulations.\n",
    "                D_loss_real, D_loss_fake = losses.discriminator_loss(D_fake, D_real)\n",
    "                D_loss = (D_loss_real + D_loss_fake) / float(config['num_D_accumulations'])\n",
    "                D_loss.backward()\n",
    "                counter += 1\n",
    "\n",
    "            # Optionally apply ortho reg in D.\n",
    "            if config['D_ortho'] > 0.0:\n",
    "                # Debug print to indicate we're using ortho reg in D.\n",
    "                print('using modified ortho reg in D')\n",
    "                utils.ortho(D, config['D_ortho'])\n",
    "\n",
    "            D.optim.step()\n",
    "\n",
    "        # Optionally toggle \"requires_grad\"\n",
    "        if config['toggle_grads']:\n",
    "            utils.toggle_grad(D, False)\n",
    "            utils.toggle_grad(G, True)\n",
    "\n",
    "        # Zero G's gradients by default before training G, for safety.\n",
    "        G.optim.zero_grad()\n",
    "\n",
    "        ###############################################################\n",
    "        #                    TRAIN GENERATOR\n",
    "        ###############################################################\n",
    "        # If accumulating gradients, loop multiple times.\n",
    "        for accumulation_index in range(config['num_G_accumulations']):\n",
    "            z_.sample_(), y_.sample_()\n",
    "            D_fake = GD(z_, y_, train_G=True, split_D=config['split_D'])\n",
    "            G_loss = losses.generator_loss(D_fake) / float(config['num_G_accumulations'])\n",
    "            G_loss.backward()\n",
    "\n",
    "        # Optionally apply modified ortho reg in G.\n",
    "        if config['G_ortho'] > 0.0:\n",
    "            print('using modified ortho reg in G')  # Debug print to indicate we're using ortho reg in G.\n",
    "            # Don't ortho reg shared, it makes no sense. Really we should blacklist any embeddings for this...\n",
    "            utils.ortho(G, config['G_ortho'],\n",
    "                        blacklist=[param for param in G.shared.parameters()])\n",
    "        G.optim.step()\n",
    "\n",
    "        # If we have an ema, update it, regardless of if we test with it or not.\n",
    "        if config['ema']:\n",
    "            ema.update(state_dict['itr'])\n",
    "\n",
    "        out = {'G_loss': float(G_loss.item()),\n",
    "               'D_loss_real': float(D_loss_real.item()),\n",
    "               'D_loss_fake': float(D_loss_fake.item())}\n",
    "        return out\n",
    "    return train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we're off!\n",
    "\n",
    "... And now we wait. At lower resolutions or fewer classes, it is possible to obtain farily respectable results in a short time frames. However, acheiving  the eye-catching results commonly advertised in paper and the media still takes quite a while, on the order of weeks potentially. \n",
    "\n",
    "Below is a table of some fairly common configurations and their expected training times:\n",
    "\n",
    "|INSERT TABLE|\n",
    "\n",
    "### \"Babysitting\" the learning process\n",
    "\n",
    "Given that training these models can be an investment in time and resources, it's wise to continuously monitor training in order to catch and address anamolies if/when they occur. Here are some things to look out for:\n",
    "\n",
    "**At the start of training**\n",
    "\n",
    "- Losses: do they fal Are the models learning? [INSERT LOSS PLOT]\n",
    "- Speed: Based on time per iteration, estimate how long training will take. Is it acceptble to you?\n",
    "- GPU utilization: Are you using GPUs to the fullest? Use command `nvidia-smi` to check on utilization and memory usage. Could you use a larger batch size? Evidence of a dataloading bottleneck? [INSERT GIF of GPU UTILs]\n",
    "\n",
    "**During training**\n",
    "- Are losses still within normal limits? High frequency oscillations are expected.\n",
    "- Monitor IS and FID metrics - are they following the expected trajectories? One of the hardest things about re-implementing a paper can be checking if the logs line up early in training, especially if training takes multiple weeks.\n",
    "- How do the samples look? Are they improving over time? Do you see evidence of mode collapse?\n",
    "- Singular Values of weights?\n",
    "\n",
    "**End of training**\n",
    "- Most importantly, do the samples meet your expectations?\n",
    "- Sharp increase in metrics followed by collapse?\n",
    "- No longer improving.\n",
    "- Explore your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of G’s progression**\n",
    "\n",
    "Remember how we saved the generator’s output on the fixed_noise batch\n",
    "after every epoch of training. Now, we can visualize the training\n",
    "progression of G with an animation. Press the play button to start the\n",
    "animation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i, (1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Real Images vs. Fake Images**\n",
    "\n",
    "Finally, lets take a look at some real images and fake images side by\n",
    "side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-339de00bc3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Grab a batch of real images from the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreal_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot the real images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,30))\n",
    "plt.subplot(2,1,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "if img_list:\n",
    "# Plot the fake images from the last epoch\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Fake Images\")\n",
    "    plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading off variety and fidelity with the \"truncation trick\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'netG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4735f1393a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'netG' is not defined"
     ]
    }
   ],
   "source": [
    "# Intra-class (z only) Latent space interpolation\n",
    "num_samples = 4\n",
    "num_midpoints = 8\n",
    "label = 1\n",
    "\n",
    "dev = next(netG.parameters()).device\n",
    "x0 = torch.randn(num_samples, nz).to(dev)\n",
    "x1 = torch.randn(num_samples, nz).to(dev)\n",
    "zs = gvutils.interp(x0, x1, num_midpoints, device=dev)\n",
    "zs = zs.view(-1, zs.size(-1))\n",
    "ys = torch.ones(zs.size(0), device=device) * label\n",
    "samples = netG(zs, ys).detach()\n",
    "\n",
    "plt.figure(figsize=(15,30))\n",
    "plt.subplot(1,1,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(samples, nrow=num_midpoints + 2, padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "print(netG.class_linear.weight.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'netG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5bccd0bed940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_midpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'netG' is not defined"
     ]
    }
   ],
   "source": [
    "# Class-wise interpolation\n",
    "# Intra-class (z only) Latent space interpolation\n",
    "num_samples = 4\n",
    "num_midpoints = 8\n",
    "\n",
    "dev = next(netG.parameters()).device\n",
    "x0 = torch.randn(num_samples, nz).to(dev)\n",
    "x1 = torch.randn(num_samples, nz).to(dev)\n",
    "zs = gvutils.interp(x0, x1, num_midpoints, device=dev)\n",
    "zs = zs.view(-1, zs.size(-1))\n",
    "print('zs.size():', zs.size())\n",
    "coastal_embed = netG.shared(torch.ones(num_samples, device=dev) * 0)\n",
    "noncoastal_embed = netG.shared(torch.ones(num_samples, device=dev) * 1)\n",
    "ys = gvutils.interp(coastal_embed, noncoastal_embed, num_midpoints, device=dev)\n",
    "print(ys.shape)\n",
    "ys = ys.view(-1, ys.size(-1))\n",
    "print(ys.shape)\n",
    "\n",
    "samples = netG.generate(zs, ys).detach()\n",
    "\n",
    "plt.figure(figsize=(15,30))\n",
    "plt.subplot(1,1,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(samples, nrow=num_midpoints + 2, padding=5, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
