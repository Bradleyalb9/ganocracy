{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANocracy: A Practioner's Guide to Training GANs in 2019\n",
    "\n",
    "**Author**: `Alex Andonian`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will give a brief introduction to some of the more recent achievements of Generative Adversarial Networks (GANs), which have seen tremendous progress in their short 5 year lifespan:\n",
    "\n",
    "![GAN Progress in Image Synthesis](assets/progress.jpg)\n",
    "\n",
    "\n",
    "As impressive as these results are, training the latest state-of-the-art GANs isn't without its challenges. For example GANs are still: \n",
    " - sensitivive to structure and parameters/configuration\n",
    " - susceptible to model collapse\n",
    " - demanding on computational resources, especially for larger resolutions\n",
    " \n",
    "The goal of this tutorial is to prepare users to take on the task of training large-scale, high-resolution GANs of their own. We will begin training a simplified version of the recent BigGAN architecture and walk through some techniques for monitoring and debugging the training process. Finally, we will demonstrate how to control sample generation using a fully pretrained BigGAN.\n",
    "\n",
    "This tutorial assumes a basic understanding of GANs as well as a machine ready to run the latest release of PyTorch (1.1). For a general overview of GANs and how they work, please refer to [Insert Introduction Link]. For more detailed instructions on how to install PyTorch, please see these instructions [Insert Link to Setup.md].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "For the **casual observer** just curious about learning more about recent GAN developments, this notebook along with the provided links to external references should sufficient an informative document. A powerful laptop or mainstream desktop computer should be enough for you to execute most of the computationally inexpensive (e.g. inference with pretrained generators).\n",
    "\n",
    "For the **interested practioner** keen on running the full notebook and playing around with different datasets and hyperparameters, a powerful desktop/server with at least one modern GPU is *highly* recommended, if not required. The median recommendation is 4-8 GPUs.\n",
    "\n",
    "If you do not have access to the recommended computational resources, it is possible (and fairly easy) to create a sufficiently powerful Virtual Machine (VM) on one of several cloud providers. We provide brief instructions and suggestions on how to get up and running with the following providers:\n",
    "\n",
    "- [Core Scientific] TODO\n",
    "- [Google Cloud] TODO\n",
    "- [IBM Cloud] TODO\n",
    "\n",
    "Nvidia and AWS also provide excellent options, but we are not able to provide specific instructions at this time.\n",
    "\n",
    "### System Requirements\n",
    "Both Linux and Windows machines are supported, but we strongly recommend Linux for performance and compatibility reasons. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Software Requriements\n",
    "\n",
    "### Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# GANOCRACY LIB\n",
    "sys.path.append('../')\n",
    "import ganocracy\n",
    "from ganocracy.data import datasets as dset\n",
    "from ganocracy.data import transforms\n",
    "from ganocracy.utils import visualizer as vutils\n",
    "from ganocracy import metrics\n",
    "\n",
    "# NOTEBOOK-SPECIFIC IMPORTS\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    # DATA CONFIG\n",
    "    'data_root': 'data',            # Root directory where datasets are stored\n",
    "    'dataset': 'ImageNet',          # Name of dataset \n",
    "    'dataset_type': 'ImageHDF5',    # Type/format of dataset class used to load data.\n",
    "    'parallel': True,\n",
    "    'shuffle': True,                # Shuffle training data\n",
    "    'batch_size': 512,              \n",
    "    'num_workers': 8,\n",
    "    'load_in_mem': True,\n",
    "    'resolution': 64,\n",
    "    'download': True,\n",
    "    'split': 'train',\n",
    "    \n",
    "    # MODEL ARCHITECTURE\n",
    "    'G_ch': 96,\n",
    "    'D_ch': 96,\n",
    "    'G_attn': 64,\n",
    "    'D_attn': 64,\n",
    "    'hier': True,\n",
    "    'dim_z': 120,\n",
    "    'shared_dim': 128,\n",
    "    'G_init': 'ortho',\n",
    "    'D_init': 'ortho',\n",
    "    'G_n1': 'inplace_relu',\n",
    "    'D_n1': 'inplace_relu',\n",
    "    'G_eval_mode': True,\n",
    "    \n",
    "    # TRAINING CONFIG     \n",
    "    'num_D_steps': 1,\n",
    "    'num_G_accumulations': 4,\n",
    "    'num_D_accumulations': 4,\n",
    "    'G_lr': 1e-4,\n",
    "    'D_lr': 4e-4,\n",
    "    'D_B2': 0.999,\n",
    "    'G_B2': 0.999,\n",
    "\n",
    "    'ema': True,\n",
    "    'use_ema': True,\n",
    "    'ema_start': 20000,\n",
    "    'test_every': 2000,\n",
    "    'save_every': 1000,\n",
    "    'num_best_copies': 5,\n",
    "    'num_save_copies': 2,\n",
    "    'seed': 0,\n",
    "    'use_multiepoch_sampler': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In this tutorial, we will use the [Places365 dataset](http://places2.csail.mit.edu/) which contains more than 10 million images comprising 400+ unique scene categories. You can manually download the full dataset [here](http://places2.csail.mit.edu/download.html), although we provide standard PyTorch dataloaders that download, cache and preprocess several standard datasets as part of this tutorial. Therefore, preparing a dataloader can be accomplished with just a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name, root_dir=None, resolution=128, filetype='tar'):\n",
    "    if filetype == 'tar':\n",
    "        url = dset.data_urls[name]['tar']\n",
    "        data_dir = dset.load_data_from_url(url, root_dir)\n",
    "        dataset = dset.ImageFolder(root=data_dir,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.CenterCropLongEdge(),\n",
    "                                       transforms.Resize(resolution),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                            (0.5, 0.5, 0.5))\n",
    "                                    ]))\n",
    "    elif filetype == 'hdf5':\n",
    "        url = dset.data_urls[name]['hdf5'][resolution]\n",
    "        hdf5_file = dset.load_data_from_url(url, root_dir)\n",
    "        dataset = dset.ImageHDF5(hdf5_file)\n",
    "    else:\n",
    "        raise ValueError('Unreconized filetype: {}'.format(filetype))\n",
    "    return dataset\n",
    "\n",
    "# dataset = get_dataset(config['dataset'],\n",
    "#                       root_dir=config['data_root'],\n",
    "#                       resolution=config['resolution'])\n",
    "\n",
    "# dataloader = torch.utils.data.DataLoader(dataset,\n",
    "#                                          shuffle=config['shuffle'],\n",
    "#                                          batch_size=config['batch_size'],\n",
    "#                                          num_workers=config['num_workers'])\n",
    "# vutils.visualize_data(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolution = config['resolution']\n",
    "# dataset = torchvision.datasets.ImageNet(config['data_root'],\n",
    "#                                         split=config['split'],\n",
    "#                                         download=config['download'],\n",
    "#                                         transform=transforms.Compose([\n",
    "#                                             transforms.CenterCropLongEdge(),\n",
    "#                                             transforms.Resize(resolution),\n",
    "#                                             transforms.ToTensor(),\n",
    "#                                             transforms.Normalize((0.5, 0.5, 0.5),\n",
    "#                                                                  (0.5, 0.5, 0.5))\n",
    "#                                     ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will need to organize and preprocess the training data that will serve as the 'real' examples presented to the discriminator. If you are training an unconditional GAN with only a single class of images, it would be enough to simply store all of your examples in a directory like so:\n",
    "\n",
    "    satellite_images/34_-77_2016-02-28.png\n",
    "    satellite_images/34_-77_2014-06-29.png\n",
    "    satellite_images/36_-75_2008-07-13.png\n",
    "    satellite_images/30_-84_2010-07-11.png\n",
    "    satellite_images/27_-97_2010-07-18.png\n",
    "    \n",
    "If you are training a class-conditional GAN such as SA-GAN or BigGAN, it is fairly common to store images organized as so:\n",
    "\n",
    "    root/dogball/xxx.png\n",
    "    root/dogball/xxy.png\n",
    "    root/dogball/xxz.png\n",
    "\n",
    "    root/cat/123.png\n",
    "    root/cat/nsdf3.png\n",
    "    root/cat/asd932_.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data as single HDF5 file (optional, for additional performance)\n",
    "\n",
    "If your dataset consists of large, high-resolution images, then repeatedly applying transforms to the raw images (cropping, resizing) results in many wasted CPU cycles. Furthermore, increasing your batchsize puts additional I/O strain on your filesystem. Together, these factors may produce a dataloading bottleneck where your GPUs consume data faster than your system can produce it. To remedy this, you may choose to prepare a pre-processed HDF5 version of your target dataset using the utilities provided. Moreover, if I/O still appears to be the bottleneck, you may choose to load the entire dataset into RAM by setting `load_in_mem` dataset kwargs to `True` (if your system can support this).\n",
    "\n",
    "Another advantage to using a single, preprocessed HDF5 is the ease with which you can transfer data to multiple remote machines, which normally becomes cumbersome and time consuming when attempting large, distributed experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_name = '{}-{}.hdf5'.format(config['dataset'], config['resolution'])\n",
    "print(hdf5_name)\n",
    "hdf5_file = os.path.join(config['data_root'], hdf5_name)\n",
    "print(hdf5_file)\n",
    "# hdf5_file = dset.make_hdf5(dataloader, config['data_root'], hdf5_name)\n",
    "dataset = dset.ImageHDF5(hdf5_file, load_in_mem=config['load_in_mem'])\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         shuffle=config['shuffle'],\n",
    "                                         batch_size=config['batch_size'],\n",
    "                                         num_workers=config['num_workers'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to measure sample quality during training\n",
    "\n",
    "Typically, when training any sort of neural network, it is standard practice to monitor the value of the objective function (loss) throughout the course of the experiment. With GANs, it is also common to break the loss into individual components.\n",
    "\n",
    "Objectively evaluating implicit generative models is difficult (Theis et al., 2015). A variety of works\n",
    "have proposed heuristics for measuring the sample quality of models without tractable likelihoods\n",
    "(Salimans et al., 2016; Heusel et al., 2017; Binkowski et al., 2018; Wu et al., 2017). Of these, ´\n",
    "the Inception Score (IS, Salimans et al. (2016)) and Frechet Inception Distance (FID, Heusel et al. ´\n",
    "(2017)) have become popular despite their notable flaws (Barratt & Sharma, 2018). We employ\n",
    "them as approximate measures of sample quality, and to enable comparison against previous work.\n",
    "\n",
    "While training, it would be very useful to objectively measure the quality of generated samples in order to understand how training is progressing (without having to manually assign subjective ratings to a large. Moreover,  To this end, researchers have developed metrics which seek to capture the quality of the generated samples, with the two most popular being:\n",
    "- Inception Score (IS)\n",
    "- Frechet Distance (FID)\n",
    "\n",
    "\n",
    "Higher IS values mean better image quality, but not necessarily diversity.\n",
    "Lower FID values mean better image quality and diversity.\n",
    "\n",
    "or datasets other than ImageNet, Inception Score can be a very poor measure of quality, so you will likely want to use --which_best FID instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_moments_file = metrics.calculate_inception_moments(dataloader, config['data_root'], config['dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "\n",
    "## Convolutional GANS \n",
    "Structure well suited for image generatation.\n",
    "When applied to images, G and D are usually convolutional neural networks (Radford et al., 2016).\n",
    "![DCGAN Generator Architecure](assets/dcgan_generator.png)\n",
    "\n",
    "Almost all GANs used for image synthesis now follow some variation of DCGAN.\n",
    "- GBlock\n",
    "- DBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward method of GBlock.\n",
    "        \n",
    "        This block increases the spatial resolution by 2:\n",
    "        \n",
    "            input:  [batch_size, in_channels, H, W]\n",
    "            output: [batch_size, out_channels, 2*H, 2*W]\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.conv(input)\n",
    "        x = self.bn(x)\n",
    "        out = self.act(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward method of DBlock.\n",
    "        \n",
    "        This block decreases the spatial resolution by 2:\n",
    "        \n",
    "            input:  [batch_size, in_channels, H, W]\n",
    "            output: [batch_size, out_channels, H/2, W/2]\n",
    "        \"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GANs\n",
    "Produce images of \n",
    "\n",
    "consider swapping ccbn and SN\n",
    "\n",
    "## Spectral Normalization (Stability)\n",
    "\n",
    "**Problem**: GAN training is highly dynamic (\n",
    "- Class-Conditional Batch Norm\n",
    "- G_EMA\n",
    "- Self-Attention\n",
    "\n",
    "(**Note**: Numerous t\n",
    "\n",
    "## Self-Attention Mechanism (Diversity)\n",
    "\n",
    "First proposed by the GANfather, Ian Goodfellow himself, in the paper Self-Attention Generative Adversarial Networks (SA-GANs), the introduction of a self-attention mechanism to GANs is aimed at\n",
    "\n",
    "### Observation and Inspiration\n",
    "Prior to SA-GANs, researchers noticed that while vanilla DCGAN-style GANs continued to improve on datasets with limited number of classes (such as faces), they still stuggled to learn the image distributions of diverse multi-class datasets like Imagenet. \n",
    "\n",
    "## two-timescale update rule (TTUR)\n",
    "\n",
    "## Scaling Up GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (linear): Linear(in_features=120, out_features=8192, bias=True)\n",
      "  (GBlocks): Sequential(\n",
      "    (0): GBlock(\n",
      "      (conv): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (1): GBlock(\n",
      "      (conv): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (2): GBlock(\n",
      "      (conv): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (3): GBlock(\n",
      "      (conv): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (out): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "output shape: torch.Size([10, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Generator Code\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    # Maps output resoluton to number of GBlocks.\n",
    "    res2blocks = {\n",
    "        32: 3,\n",
    "        64: 4,\n",
    "        128: 5,\n",
    "        256: 6,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, dim_z=128, resolution=128, G_ch=64, \n",
    "                block=GBlock):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.G_ch = G_ch\n",
    "        self.dim_z = dim_z\n",
    "\n",
    "        self.num_blocks = self.res2blocks[resolution]\n",
    "        self.ch_nums = [2**i for i in range(self.num_blocks)]\n",
    "        self.ch_nums += self.ch_nums[-1:]\n",
    "        self.ch_nums = list(reversed(self.ch_nums))\n",
    "        \n",
    "        self.linear = nn.Linear(dim_z, G_ch * self.ch_nums[0] * 4**2)\n",
    "        self.GBlocks = nn.Sequential(*[\n",
    "            block(G_ch * in_c, G_ch * out_c)\n",
    "            for in_c, out_c in zip(self.ch_nums, self.ch_nums[1:])\n",
    "        ])\n",
    "        self.out = nn.Conv2d(G_ch * 1, 3, 3, padding=1)  # RGB image has 3 channels\n",
    "        self.tanh = nn.Tanh()                            # \"Squashes\" out to be in range[-1, 1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(x.size(0), -1, 4, 4)\n",
    "        x = self.GBlocks(x)\n",
    "        return self.tanh(self.out(x))\n",
    "    \n",
    "dim_z = 128\n",
    "z = torch.rand(10, config['dim_z'])\n",
    "G = Generator(dim_z=config['dim_z'], resolution=config['resolution'])\n",
    "print(G)\n",
    "print('output shape:', G(z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (input_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (DBlocks): Sequential(\n",
      "    (0): DBlock(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    )\n",
      "    (1): DBlock(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    )\n",
      "    (2): DBlock(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    )\n",
      "  )\n",
      "  (out): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (act): Sigmoid()\n",
      ")\n",
      "output shape: torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    # Maps output resoluton to number of DBlocks.\n",
    "    res2blocks = {\n",
    "        32: 3,\n",
    "        64: 4,\n",
    "        128: 5,\n",
    "        256: 6,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, resolution=128, D_ch=64, block=DBlock):\n",
    "        super().__init__()\n",
    "        self.D_ch = D_ch\n",
    "        self.num_blocks = self.res2blocks[resolution]\n",
    "        self.fnums = [2**i for i in range(self.num_blocks)]\n",
    "        self.input_layer = nn.Conv2d(3, D_ch, 3, padding=1)\n",
    "        \n",
    "        self.DBlocks = nn.Sequential(*[\n",
    "            block(D_ch * in_c, D_ch * out_c)\n",
    "            for in_c, out_c in zip(self.fnums, self.fnums[1:])\n",
    "        ])\n",
    "        \n",
    "        self.out = nn.Conv2d(D_ch * self.fnums[-1], 1, 3, 1, 0)\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.DBlocks(x)\n",
    "        x = self.act(torch.mean(self.out(x), [2, 3]))\n",
    "        return x\n",
    "\n",
    "\n",
    "x = torch.rand(10, 3, config['resolution'], config['resolution'])\n",
    "D = Discriminator(resolution=config['resolution'])\n",
    "print(D)\n",
    "print('output shape:', D(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_training_function(G, D, GD, z_, y_, ema, state_dict, config):\n",
    "    def train(x, y):\n",
    "        G.optim.zero_grad()\n",
    "        D.optim.zero_grad()\n",
    "        # How many chunks to split x and y into?\n",
    "        x = torch.split(x, config['batch_size'])\n",
    "        y = torch.split(y, config['batch_size'])\n",
    "        counter = 0\n",
    "\n",
    "        # Optionally toggle D and G's \"require_grad\"\n",
    "        if config['toggle_grads']:\n",
    "            utils.toggle_grad(D, True)\n",
    "            utils.toggle_grad(G, False)\n",
    "\n",
    "        ###############################################################\n",
    "        #                    TRAIN DISCRIMINATOR\n",
    "        ###############################################################\n",
    "        for step_index in range(config['num_D_steps']):\n",
    "            # If accumulating gradients, loop multiple times before an optimizer step\n",
    "            for accumulation_index in range(config['num_D_accumulations']):\n",
    "                z_.sample_(), y_.sample_()\n",
    "                D_fake, D_real = GD(z_[:config['batch_size']], y_[:config['batch_size']],\n",
    "                                    x[counter], y[counter], train_G=False,\n",
    "                                    split_D=config['split_D'])\n",
    "\n",
    "                # Compute components of D's loss, average them, and divide by\n",
    "                # the number of gradient accumulations.\n",
    "                D_loss_real, D_loss_fake = losses.discriminator_loss(D_fake, D_real)\n",
    "                D_loss = (D_loss_real + D_loss_fake) / float(config['num_D_accumulations'])\n",
    "                D_loss.backward()\n",
    "                counter += 1\n",
    "\n",
    "            # Optionally apply ortho reg in D.\n",
    "            if config['D_ortho'] > 0.0:\n",
    "                # Debug print to indicate we're using ortho reg in D.\n",
    "                print('using modified ortho reg in D')\n",
    "                utils.ortho(D, config['D_ortho'])\n",
    "\n",
    "            D.optim.step()\n",
    "\n",
    "        # Optionally toggle \"requires_grad\"\n",
    "        if config['toggle_grads']:\n",
    "            utils.toggle_grad(D, False)\n",
    "            utils.toggle_grad(G, True)\n",
    "\n",
    "        # Zero G's gradients by default before training G, for safety.\n",
    "        G.optim.zero_grad()\n",
    "\n",
    "        ###############################################################\n",
    "        #                    TRAIN GENERATOR\n",
    "        ###############################################################\n",
    "        # If accumulating gradients, loop multiple times.\n",
    "        for accumulation_index in range(config['num_G_accumulations']):\n",
    "            z_.sample_(), y_.sample_()\n",
    "            D_fake = GD(z_, y_, train_G=True, split_D=config['split_D'])\n",
    "            G_loss = losses.generator_loss(D_fake) / float(config['num_G_accumulations'])\n",
    "            G_loss.backward()\n",
    "\n",
    "        # Optionally apply modified ortho reg in G.\n",
    "        if config['G_ortho'] > 0.0:\n",
    "            print('using modified ortho reg in G')  # Debug print to indicate we're using ortho reg in G.\n",
    "            # Don't ortho reg shared, it makes no sense. Really we should blacklist any embeddings for this...\n",
    "            utils.ortho(G, config['G_ortho'],\n",
    "                        blacklist=[param for param in G.shared.parameters()])\n",
    "        G.optim.step()\n",
    "\n",
    "        # If we have an ema, update it, regardless of if we test with it or not.\n",
    "        if config['ema']:\n",
    "            ema.update(state_dict['itr'])\n",
    "\n",
    "        out = {'G_loss': float(G_loss.item()),\n",
    "               'D_loss_real': float(D_loss_real.item()),\n",
    "               'D_loss_fake': float(D_loss_fake.item())}\n",
    "        return out\n",
    "    return train\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
